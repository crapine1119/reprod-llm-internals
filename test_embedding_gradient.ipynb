{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:21:47.841216Z",
     "start_time": "2025-12-16T03:21:43.263312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from accelerate import init_empty_weights\n",
    "import json\n",
    "\n",
    "# 모델 로드\n",
    "# model_cache_fp = \"/Users/songhak/.cache/huggingface/hub/models--yanolja--EEVE-Korean-Instruct-2.8B-v1.0/snapshots/482db2d0ba911253d09342c34d0e42ac871bfea3\"\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-7B\"\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(\"Model weight are being loaded...\")\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_cache_fp)"
   ],
   "id": "40caa933b1781927",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3adf89bf0d594fe38b4e8311c5c6a20d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b81ddc80727a4fc09850a29c098085d8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "917144deade14f0c84c4f4d6ac7e57aa"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8443e0317ec04c0b8c35c365be5555b5"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight are being loaded...\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:22:01.069870Z",
     "start_time": "2025-12-16T03:22:00.329248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gradient zero masking\n",
    "from random import sample\n",
    "import re\n",
    "vocab = tokenizer.get_vocab()  # {token: id}\n",
    "korean_ids = []\n",
    "examples = []\n",
    "for tok, idx in vocab.items():\n",
    "    # 아주 러프하게: 토큰 문자열에 한글 문자가 하나라도 있으면 한국어 토큰으로 간주\n",
    "    decoded = tokenizer.decode([idx])\n",
    "    if any(re.findall(\"[ㄱ-ㅣ,가-힣]\", decoded)):\n",
    "        korean_ids.append(idx)\n",
    "        examples.append(\"\".join(decoded))\n",
    "\n",
    "korean_ids = sorted(set(korean_ids))\n",
    "emb = model.get_input_embeddings()  # nn.Embedding\n",
    "\n",
    "# check ratio\n",
    "print(f\"{len(korean_ids) / len(vocab) * 100:0.2f}% are KOR tokens: Samples ({sample(examples, 5)})\")\n",
    "\n",
    "vocab_size, dim = emb.weight.shape\n",
    "mask = torch.zeros_like(emb.weight, dtype=torch.bool)\n",
    "mask[korean_ids] = True            # 한국어 토큰 row만 True\n",
    "\n",
    "def grad_mask_hook(grad):\n",
    "    # grad: [vocab_size, dim]\n",
    "    # 한국어 토큰이 아닌 row의 gradient 0으로\n",
    "    return grad * mask.to(grad.device)\n",
    "\n",
    "emb.weight.register_hook(grad_mask_hook)\n",
    "\n",
    "# how to test simply?"
   ],
   "id": "6796b3f4203b58fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.46% are KOR tokens: Samples ([' 않고', ' 수정', ' 일', ' 있음', ' 전문'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x11615b950>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:22:25.036079Z",
     "start_time": "2025-12-16T03:22:25.026082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# split embedding\n",
    "class SplitEmbedding(nn.Module):\n",
    "    def __init__(self, base_weight: torch.Tensor, num_new_tokens: int):\n",
    "        super().__init__()\n",
    "        # base_weight: [base_vocab_size, dim] (이미 학습된 weight)\n",
    "        self.base_vocab_size, self.dim = base_weight.shape\n",
    "\n",
    "        # 기존 embedding은 gradient 없는 buffer로 등록\n",
    "        self.register_buffer(\"embed_base\", base_weight)  # [V_base, D]\n",
    "\n",
    "        # 새 한국어 토큰 부분만 trainable embedding\n",
    "        self.embed_new = nn.Embedding(num_new_tokens, self.dim)\n",
    "        nn.init.normal_(self.embed_new.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # input_ids: [B, T]\n",
    "        device = input_ids.device\n",
    "        out = torch.empty(\n",
    "            (*input_ids.shape, self.dim),\n",
    "            device=device,\n",
    "            dtype=self.embed_base.dtype,\n",
    "        )\n",
    "\n",
    "        base_mask = input_ids < self.base_vocab_size\n",
    "        new_mask = ~base_mask\n",
    "\n",
    "        if base_mask.any():\n",
    "            base_ids = input_ids[base_mask]      # 1D tensor\n",
    "            out[base_mask] = self.embed_base[base_ids]\n",
    "\n",
    "        if new_mask.any():\n",
    "            new_ids = input_ids[new_mask] - self.base_vocab_size\n",
    "            out[new_mask] = self.embed_new(new_ids)\n",
    "\n",
    "        return out"
   ],
   "id": "a3f6a6e0253b1e32",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:23:12.717160Z",
     "start_time": "2025-12-16T03:23:09.786618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# new model to merge\n",
    "print(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(\"Model weight are being loaded...\")\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "\n",
    "def to_scalable_params(x: int) -> str:\n",
    "    if x > 1000 ** 3:\n",
    "        return f\"{x / 1000**3:.2f}B\"\n",
    "    return f\"{x / 1000**2:.2f}M\"\n",
    "\n",
    "def to_scalable_bytes(x: int) -> str:\n",
    "    if x > 1024 ** 3:\n",
    "        return f\"{x / 1024**3:.2f}GiB\"\n",
    "    return f\"{x / 1024**2:.2f}MiB\"\n",
    "\n",
    "\n",
    "def count_params(module: nn.Module) -> tuple[int, int]:\n",
    "    total = 0\n",
    "    trainable = 0\n",
    "    for p in module.parameters():\n",
    "        n = p.numel()\n",
    "        total += n\n",
    "        if p.requires_grad:\n",
    "            trainable += n\n",
    "\n",
    "    return to_scalable_params(total), to_scalable_params(trainable)\n",
    "\n",
    "def module_param_stats(module: torch.nn.Module) -> list[dict]:\n",
    "    \"\"\"\n",
    "    leaf 모듈(자식 모듈이 없는 모듈) 단위로\n",
    "    파라미터 개수 / 메모리 합을 계산.\n",
    "    \"\"\"\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    for module_name, module in module.named_modules():\n",
    "        # 자식이 있는 모듈은 스킵하고 leaf만 본다.\n",
    "        if any(True for _ in module.children()):\n",
    "            continue\n",
    "\n",
    "        num_params = 0\n",
    "        num_trainable = 0\n",
    "        num_bytes = 0\n",
    "        for name, p in module.named_parameters(recurse=False):\n",
    "            n = p.numel()\n",
    "            num_params += n\n",
    "            if p.requires_grad:\n",
    "                num_trainable += n\n",
    "            num_bytes += n * p.element_size()\n",
    "\n",
    "        if num_params == 0:\n",
    "            continue\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"name\": module_name or \"(root)\",\n",
    "                \"num_params\": num_params,\n",
    "                \"num_trainable\": num_trainable,\n",
    "                \"num_bytes\": num_bytes,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # 파라미터 개수 기준 내림차순 정렬\n",
    "    rows.sort(key=lambda r: r[\"num_params\"], reverse=True)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def group_by_name_prefix(\n",
    "    rows: list[dict],\n",
    "    prefixes: dict[str, list[str]],\n",
    ") -> dict[str, dict]:\n",
    "    \"\"\"\n",
    "    이름 prefix로 대략적인 그룹을 만들어 합산.\n",
    "    예: embedding, transformer block, lm_head 등\n",
    "    \"\"\"\n",
    "    stats: dict[str, dict] = {}\n",
    "    for group_name in prefixes.keys():\n",
    "        stats[group_name] = {\n",
    "            \"num_params\": 0,\n",
    "            \"num_trainable\": 0,\n",
    "            \"num_bytes\": 0,\n",
    "        }\n",
    "    stats[\"(others)\"] = {\"num_params\": 0, \"num_trainable\": 0, \"num_bytes\": 0}\n",
    "\n",
    "    for row in rows:\n",
    "        name = row[\"name\"]\n",
    "        matched_group = None\n",
    "        for group_name, pfx_list in prefixes.items():\n",
    "            if any(name.startswith(pfx) for pfx in pfx_list):\n",
    "                matched_group = group_name\n",
    "                break\n",
    "        if matched_group is None:\n",
    "            matched_group = \"(others)\"\n",
    "\n",
    "        for key in [\"num_params\", \"num_trainable\", \"num_bytes\"]:\n",
    "            stats[matched_group][key] += row[key]\n",
    "    for group_name in stats.keys():\n",
    "        for key in [\"num_params\", \"num_trainable\"]:\n",
    "            stats[group_name][key] = to_scalable_params(stats[group_name][key])\n",
    "        stats[group_name][\"num_bytes\"] = to_scalable_bytes(stats[group_name][\"num_bytes\"])\n",
    "\n",
    "    return stats\n",
    "\n",
    "# print(count_params(model))\n",
    "rows = module_param_stats(model)\n",
    "result = group_by_name_prefix(rows, {\"embed\": [\"model.embed\", \"model.rotary_emb\"], \"fc\": [\"lm_head\"], \"layers\": [\"model.layers\"]})\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "\n",
    "# N개 토큰이 추가되었다고 가정\n",
    "assumed_added_num = 50000\n",
    "base_vocab_size = vocab_size - assumed_added_num"
   ],
   "id": "71f52e9cc30e416b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "Model weight are being loaded...\n",
      "{\n",
      "  \"embed\": {\n",
      "    \"num_params\": \"525.34M\",\n",
      "    \"num_trainable\": \"525.34M\",\n",
      "    \"num_bytes\": \"1002.00MiB\"\n",
      "  },\n",
      "  \"fc\": {\n",
      "    \"num_params\": \"525.34M\",\n",
      "    \"num_trainable\": \"525.34M\",\n",
      "    \"num_bytes\": \"1002.00MiB\"\n",
      "  },\n",
      "  \"layers\": {\n",
      "    \"num_params\": \"6.98B\",\n",
      "    \"num_trainable\": \"6.98B\",\n",
      "    \"num_bytes\": \"13.00GiB\"\n",
      "  },\n",
      "  \"(others)\": {\n",
      "    \"num_params\": \"0.00M\",\n",
      "    \"num_trainable\": \"0.00M\",\n",
      "    \"num_bytes\": \"0.01MiB\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:23:22.453651Z",
     "start_time": "2025-12-16T03:23:17.022398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 추가된 임베딩 세팅\n",
    "old_emb = model.get_input_embeddings()  # nn.Embedding\n",
    "old_weight = old_emb.weight.data.clone()  # [base_vocab_size, dim] 만 쓰고 싶으면 slice\n",
    "\n",
    "# 혹시 resize_token_embeddings를 이미 호출했다면, base 부분만 잘라야 함\n",
    "old_weight_base = old_weight[:base_vocab_size]\n",
    "\n",
    "split_emb = SplitEmbedding(old_weight_base, num_new_tokens=assumed_added_num)\n",
    "split_emb.to(model.device)  # model과 같은 디바이스로\n",
    "model.set_input_embeddings(split_emb)\n"
   ],
   "id": "cb43d465a6309fde",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:23:26.090390Z",
     "start_time": "2025-12-16T03:23:26.081789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SplitLMHead(nn.Module):\n",
    "    def __init__(self, base_weight: torch.Tensor, new_embed: nn.Embedding):\n",
    "        super().__init__()\n",
    "        # base_weight: [V_base, D]\n",
    "        self.vocab_size_base, self.dim = base_weight.shape\n",
    "\n",
    "        # base 쪽은 buffer (고정)\n",
    "        # [D, V_base] 로 transpose 해두면 matmul에 편함\n",
    "        self.register_buffer(\"lm_base\", base_weight.T)\n",
    "\n",
    "        # 새 토큰 부분은 input embedding의 embed_new와 tie 하고 싶다면:\n",
    "        self.embed_new = new_embed  # nn.Embedding\n",
    "        # 별도의 Linear를 둘 수도 있고, F.linear로 weight 공유만 할 수도 있음\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: [B, T, D]\n",
    "        B, T, D = hidden_states.shape\n",
    "\n",
    "        # base logits\n",
    "        logits_base = hidden_states @ self.lm_base  # [B, T, V_base]\n",
    "\n",
    "        # new token logits: F.linear(hidden, W_new^T)\n",
    "        logits_new = torch.matmul(\n",
    "            hidden_states,\n",
    "            self.embed_new.weight.T,  # [D, V_new]\n",
    "        )  # [B, T, V_new]\n",
    "\n",
    "        # 최종 logits concat\n",
    "        logits = torch.cat([logits_base, logits_new], dim=-1)\n",
    "        return logits\n"
   ],
   "id": "1a3ca37680f29380",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:23:28.833668Z",
     "start_time": "2025-12-16T03:23:28.823729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shared weight\n",
    "split_lm = SplitLMHead(\n",
    "    base_weight=old_weight_base,  # or old_lm_head_weight[:base_vocab_size]\n",
    "    new_embed=model.get_input_embeddings().embed_new,\n",
    ")\n",
    "model.lm_head = split_lm"
   ],
   "id": "a191663edffbdd9d",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:23:31.511130Z",
     "start_time": "2025-12-16T03:23:31.500933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# turn base params grad off\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "for p in model.get_input_embeddings().embed_new.parameters():\n",
    "    p.requires_grad = True\n",
    "# SplitLMHead는 embed_new만 trainable이면 별도 param은 없음 (또는 있으면 같이 추가)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.get_input_embeddings().embed_new.parameters(),\n",
    "    lr=1e-3,\n",
    ")"
   ],
   "id": "34d13c06a20a87c2",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:23:33.537111Z",
     "start_time": "2025-12-16T03:23:33.519367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 메모리 차이 계산\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "DTYPE_BYTES = {\n",
    "    \"fp32\": 4,\n",
    "    \"float32\": 4,\n",
    "    \"fp16\": 2,\n",
    "    \"float16\": 2,\n",
    "    \"bf16\": 2,\n",
    "    \"bfloat16\": 2,\n",
    "}\n",
    "\n",
    "def to_mib(x: int) -> float:\n",
    "    return x / 1024**2\n",
    "\n",
    "def to_gib(x: int) -> float:\n",
    "    return x / 1024**3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingMemory:\n",
    "    vocab: int\n",
    "    dim: int\n",
    "    param_bytes: int\n",
    "    grad_bytes: int\n",
    "    opt_bytes: int  # optimizer state (Adam m, v 등)\n",
    "    bytes_per_param: int\n",
    "\n",
    "    @property\n",
    "    def total_bytes(self) -> int:\n",
    "        return self.param_bytes + self.grad_bytes + self.opt_bytes\n",
    "\n",
    "    def pretty(self, title: str = \"\"):\n",
    "        if title:\n",
    "            print(f\"=== {title} ===\")\n",
    "        print(f\"- vocab          : {self.vocab}\")\n",
    "        print(f\"- dim            : {self.dim}\")\n",
    "        print(f\"- bytes/param    : {self.bytes_per_param} B\")\n",
    "        print(f\"- param   : {to_gib(self.param_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.param_bytes):8.1f} MiB)\")\n",
    "        print(f\"- grad    : {to_gib(self.grad_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.grad_bytes):8.1f} MiB)\")\n",
    "        print(f\"- opt(m,v): {to_gib(self.opt_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.opt_bytes):8.1f} MiB)\")\n",
    "        print(f\"- TOTAL   : {to_gib(self.total_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.total_bytes):8.1f} MiB)\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def calc_embedding_memory(\n",
    "    vocab: int,\n",
    "    dim: int,\n",
    "    dtype_param: str = \"fp16\",\n",
    "    dtype_grad: str = \"fp16\",\n",
    "    dtype_moment: str = \"fp32\",\n",
    "    optimizer: str = \"adam\",\n",
    ") -> EmbeddingMemory:\n",
    "    \"\"\"\n",
    "    embedding 하나에 대해\n",
    "      - 파라미터 메모리\n",
    "      - gradient 메모리\n",
    "      - optimizer state(Adam m,v) 메모리\n",
    "    를 계산합니다.\n",
    "    \"\"\"\n",
    "    n_params = vocab * dim\n",
    "\n",
    "    bytes_param = DTYPE_BYTES[dtype_param]\n",
    "    bytes_grad = DTYPE_BYTES[dtype_grad]\n",
    "    bytes_moment = DTYPE_BYTES[dtype_moment]\n",
    "\n",
    "    param_bytes = n_params * bytes_param\n",
    "    grad_bytes = n_params * bytes_grad\n",
    "\n",
    "    if optimizer.lower() in (\"adam\", \"adamw\"):\n",
    "        # Adam: 1차 모멘트 m, 2차 모멘트 v 두 개\n",
    "        opt_bytes = n_params * bytes_moment * 2\n",
    "    else:\n",
    "        # 필요 시 다른 optimizer 로직 추가 가능\n",
    "        opt_bytes = 0\n",
    "\n",
    "    return EmbeddingMemory(\n",
    "        vocab=vocab,\n",
    "        dim=dim,\n",
    "        param_bytes=param_bytes,\n",
    "        grad_bytes=grad_bytes,\n",
    "        opt_bytes=opt_bytes,\n",
    "        bytes_per_param=bytes_param,\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SplitEmbeddingMemory:\n",
    "    total_vocab: int\n",
    "    kor_vocab: int\n",
    "    dim: int\n",
    "    baseline: EmbeddingMemory\n",
    "    split: EmbeddingMemory  # \"학습되는 부분(한국어 토큰)\"에 대한 grad/state만\n",
    "\n",
    "    @property\n",
    "    def split_param_bytes_total(self) -> int:\n",
    "        \"\"\"\n",
    "        Split 구조에서:\n",
    "        - 파라미터(embedding weight)는 전체 vocab에 대해 존재\n",
    "          (base 부분은 buffer, kor 부분은 trainable)\n",
    "        - baseline과 같은 dtype을 사용한다고 가정\n",
    "        \"\"\"\n",
    "        bytes_per_param = self.baseline.bytes_per_param\n",
    "        return self.total_vocab * self.dim * bytes_per_param\n",
    "\n",
    "    @property\n",
    "    def split_total_bytes(self) -> int:\n",
    "        \"\"\"\n",
    "        Split 구조에서의 총 메모리:\n",
    "        - 파라미터: 전체 vocab\n",
    "        - grad/state: 한국어 vocab 부분만\n",
    "        \"\"\"\n",
    "        return self.split_param_bytes_total + self.split.grad_bytes + self.split.opt_bytes\n",
    "\n",
    "    @property\n",
    "    def saved_bytes(self) -> int:\n",
    "        return self.baseline.total_bytes - self.split_total_bytes\n",
    "\n",
    "    @property\n",
    "    def split_ratio(self) -> float:\n",
    "        \"\"\"\n",
    "        Split 구조 총 메모리 / Baseline 총 메모리\n",
    "        (즉, 기존 대비 몇 % 메모리만 쓰는지)\n",
    "        \"\"\"\n",
    "        return self.split_total_bytes / self.baseline.total_bytes\n",
    "\n",
    "    @property\n",
    "    def saved_ratio(self) -> float:\n",
    "        \"\"\"\n",
    "        절약 비율: 1 - split_ratio\n",
    "        (기존 대비 얼마나 줄었는지)\n",
    "        \"\"\"\n",
    "        return 1.0 - self.split_ratio\n",
    "\n",
    "    def pretty(self):\n",
    "        print(\"====== Baseline: 전체 vocab 학습 ======\")\n",
    "        self.baseline.pretty(\"Baseline (all tokens trainable)\")\n",
    "\n",
    "        print(\"====== Split: 한국어 토큰만 학습 ======\")\n",
    "        print(f\"- total_vocab : {self.total_vocab}\")\n",
    "        print(f\"- kor_vocab   : {self.kor_vocab}\")\n",
    "        print(f\"- dim         : {self.dim}\")\n",
    "        print()\n",
    "\n",
    "        print(\">> 한국어 토큰 부분(trainable embedding)만 기준으로 한 메모리\")\n",
    "        self.split.pretty(\"Trainable Korean part only\")\n",
    "\n",
    "        print(\">> 실제 Split 구조에서의 총 메모리 추정\")\n",
    "        print(f\"- 파라미터(전체 vocab) : \"\n",
    "              f\"{to_gib(self.split_param_bytes_total):7.3f} GiB \"\n",
    "              f\"({to_mib(self.split_param_bytes_total):8.1f} MiB)\")\n",
    "        print(f\"- grad(한국어 토큰만) : \"\n",
    "              f\"{to_gib(self.split.grad_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.split.grad_bytes):8.1f} MiB)\")\n",
    "        print(f\"- opt (한국어 토큰만) : \"\n",
    "              f\"{to_gib(self.split.opt_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.split.opt_bytes):8.1f} MiB)\")\n",
    "        print(f\"- TOTAL(SPLIT)        : \"\n",
    "              f\"{to_gib(self.split_total_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.split_total_bytes):8.1f} MiB)\")\n",
    "        print()\n",
    "\n",
    "        # 절약량 (절대값)\n",
    "        print(\">> 절감된 메모리 (Baseline - Split)\")\n",
    "        print(f\"- saved bytes : {to_gib(self.saved_bytes):7.3f} GiB \"\n",
    "              f\"({to_mib(self.saved_bytes):8.1f} MiB)\")\n",
    "        print()\n",
    "\n",
    "        # 비율 (질문하신 부분)\n",
    "        print(\">> Baseline 대비 Split 메모리 비율\")\n",
    "        print(f\"- Split / Baseline : {self.split_ratio * 100:5.2f}%\")\n",
    "        print(f\"- Saved            : {self.saved_ratio * 100:5.2f}% 감소\")\n",
    "        print()\n"
   ],
   "id": "54bd82ca1256e98b",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:23:39.730142Z",
     "start_time": "2025-12-16T03:23:39.698636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# total_vocab = vocab_size\n",
    "# kor_vocab = assumed_added_num\n",
    "# dim = old_weight_base.shape[-1]\n",
    "\n",
    "\n",
    "kor_vocab = assumed_added_num\n",
    "total_vocab = vocab_size\n",
    "dim = 1024\n",
    "\n",
    "\n",
    "dtype_param = \"fp32\"\n",
    "dtype_grad = \"fp32\"\n",
    "dtype_moment = \"fp32\"   # Adam 모멘트 fp32 가정\n",
    "optimizer = \"adamw\"\n",
    "\n",
    "\n",
    "baseline = calc_embedding_memory(\n",
    "    vocab=total_vocab,\n",
    "    dim=dim,\n",
    "    dtype_param=dtype_param,\n",
    "    dtype_grad=dtype_grad,\n",
    "    dtype_moment=dtype_moment,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "split_trainable = calc_embedding_memory(\n",
    "    vocab=kor_vocab,\n",
    "    dim=dim,\n",
    "    dtype_param=dtype_param,\n",
    "    dtype_grad=dtype_grad,\n",
    "    dtype_moment=dtype_moment,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "result = SplitEmbeddingMemory(\n",
    "        total_vocab=total_vocab,\n",
    "        kor_vocab=kor_vocab,\n",
    "        dim=dim,\n",
    "        baseline=baseline,\n",
    "        split=split_trainable,\n",
    "    )\n",
    "result.pretty()"
   ],
   "id": "a38e4be81faca693",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Baseline: 전체 vocab 학습 ======\n",
      "=== Baseline (all tokens trainable) ===\n",
      "- vocab          : 128256\n",
      "- dim            : 1024\n",
      "- bytes/param    : 4 B\n",
      "- param   :   0.489 GiB (   501.0 MiB)\n",
      "- grad    :   0.489 GiB (   501.0 MiB)\n",
      "- opt(m,v):   0.979 GiB (  1002.0 MiB)\n",
      "- TOTAL   :   1.957 GiB (  2004.0 MiB)\n",
      "\n",
      "====== Split: 한국어 토큰만 학습 ======\n",
      "- total_vocab : 128256\n",
      "- kor_vocab   : 50000\n",
      "- dim         : 1024\n",
      "\n",
      ">> 한국어 토큰 부분(trainable embedding)만 기준으로 한 메모리\n",
      "=== Trainable Korean part only ===\n",
      "- vocab          : 50000\n",
      "- dim            : 1024\n",
      "- bytes/param    : 4 B\n",
      "- param   :   0.191 GiB (   195.3 MiB)\n",
      "- grad    :   0.191 GiB (   195.3 MiB)\n",
      "- opt(m,v):   0.381 GiB (   390.6 MiB)\n",
      "- TOTAL   :   0.763 GiB (   781.2 MiB)\n",
      "\n",
      ">> 실제 Split 구조에서의 총 메모리 추정\n",
      "- 파라미터(전체 vocab) :   0.489 GiB (   501.0 MiB)\n",
      "- grad(한국어 토큰만) :   0.191 GiB (   195.3 MiB)\n",
      "- opt (한국어 토큰만) :   0.381 GiB (   390.6 MiB)\n",
      "- TOTAL(SPLIT)        :   1.061 GiB (  1086.9 MiB)\n",
      "\n",
      ">> 절감된 메모리 (Baseline - Split)\n",
      "- saved bytes :   0.896 GiB (   917.1 MiB)\n",
      "\n",
      ">> Baseline 대비 Split 메모리 비율\n",
      "- Split / Baseline : 54.24%\n",
      "- Saved            : 45.76% 감소\n",
      "\n"
     ]
    }
   ],
   "execution_count": 144
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
